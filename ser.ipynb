{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixZY5CNfG37D"
   },
   "source": [
    "### **DATA PREPROCESSING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "-HN3x8BF9RgK",
    "outputId": "e8c06481-36fc-4b92-d314-a61202cfc62d"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "import csv\n",
    "import librosa\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import zipfile\n",
    "import os\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file= \"Emotion_zipped.zip\"\n",
    "target_dir = \"Emotion\"\n",
    "\n",
    "if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir)\n",
    "\n",
    "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LYTeBcFNZXcq"
   },
   "outputs": [],
   "source": [
    "output_csv_path =\"hsr_all_test.csv\"\n",
    "emotions = ['Angry', 'Disgusted', 'Fearful', 'Happy', 'Neutral', 'Sad']\n",
    "with open(output_csv_path, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "\n",
    "    # Write the header\n",
    "    csv_writer.writerow(['Emotion', 'Path'])\n",
    "\n",
    "    # Write data to the CSV file for each emotion\n",
    "    for emotion in emotions:\n",
    "        emotion_paths = glob(f'Emotions/{emotion}/*.wav')\n",
    "        emotion_paths = sorted(emotion_paths)\n",
    "\n",
    "        for path in emotion_paths:\n",
    "            # Extract only the {emotion}/*.wav part from the full path\n",
    "            relative_path = os.path.join(emotion, os.path.basename(path))\n",
    "            csv_writer.writerow([emotion, relative_path])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N29Bfd67dU2Q"
   },
   "outputs": [],
   "source": [
    "def split_data(dataset,test_ratio=0.1,valid_ratio=0.1):\n",
    "  df=pd.read_csv(dataset)\n",
    "  shuffled_df = df.sample(frac=1).reset_index(drop=True)\n",
    "  train_index = int(len(shuffled_df)*(1-test_ratio-valid_ratio))\n",
    "\n",
    "\n",
    "  df_train = shuffled_df[:train_index]\n",
    "\n",
    "  valid_index = int(len(shuffled_df)*valid_ratio)\n",
    "\n",
    "  df_valid = shuffled_df[train_index:train_index+valid_index]\n",
    "  df_valid = df_valid.reset_index(drop=True)\n",
    "  df_test = shuffled_df[train_index+valid_index:]\n",
    "  df_test = df_test.reset_index(drop=True)\n",
    "  del df\n",
    "  gc.collect()\n",
    "  return df_train, df_valid, df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LTCYkOFef7ew"
   },
   "outputs": [],
   "source": [
    "df_train,df_valid,df_test = split_data(\"hsr_all_test.csv\")\n",
    "df_train.to_csv('df_train.csv')\n",
    "df_valid.to_csv('df_valid.csv')\n",
    "df_test.to_csv('df_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elMp8qqAc636"
   },
   "source": [
    "### **FEATURE EXTRACTION**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fdKLPUXbc6A4"
   },
   "outputs": [],
   "source": [
    "def feature_extraction(file_path):\n",
    "    y, sr = librosa.load(f\"Emotions/{file_path}\")\n",
    "    mfcc = np.array(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=64))\n",
    "    mfcc_mean = mfcc.mean(axis=1)\n",
    "    mfcc_min = mfcc.min(axis=1)\n",
    "    mfcc_max = mfcc.max(axis=1)\n",
    "    mfcc_features = np.concatenate( (mfcc_mean, mfcc_min, mfcc_max) )\n",
    "    #features=mfcc_features\n",
    "    melspectrogram = np.array(librosa.feature.melspectrogram(y=y, sr=sr))\n",
    "    melspectrogram_mean = melspectrogram.mean(axis=1)\n",
    "    melspectrogram_min = melspectrogram.min(axis=1)\n",
    "    melspectrogram_max = melspectrogram.max(axis=1)\n",
    "    melspectrogram_features = np.concatenate( (melspectrogram_mean, melspectrogram_min, melspectrogram_max) )\n",
    "    spectral_centroid = np.array(librosa.feature.spectral_centroid(y=y , sr=sr))\n",
    "    mean_centroid = spectral_centroid.mean(axis=1)\n",
    "    min_centroid = spectral_centroid.min(axis=1)\n",
    "    max_centroid = spectral_centroid.max(axis=1)\n",
    "    spectral_centroid_features = np.concatenate( (mean_centroid, min_centroid, max_centroid) )\n",
    "    features = np.concatenate( (mfcc_features,  melspectrogram_features,spectral_centroid_features) )\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RvVEU5ldGJGS"
   },
   "outputs": [],
   "source": [
    "feature_train= df_train['Path'].apply(lambda path:feature_extraction(path))\n",
    "feature_test= df_test['Path'].apply(lambda path:feature_extraction(path))\n",
    "feature_valid= df_valid['Path'].apply(lambda path:feature_extraction(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TIztTuM7ce8w"
   },
   "outputs": [],
   "source": [
    "feature_train.to_csv('train_feature.csv',index=False)\n",
    "feature_test.to_csv('test_feature.csv',index=False)\n",
    "feature_valid.to_csv('valid_feature.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SOZRHVLiEg5O"
   },
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "train_feature_df = pd.read_csv('train_feature.csv')\n",
    "test_feature_df = pd.read_csv('test_feature.csv')\n",
    "valid_feature_df = pd.read_csv('valid_feature.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zh1KUrIr2len"
   },
   "outputs": [],
   "source": [
    "feature_train = train_feature_df['Path'].apply(lambda x: np.fromstring(x[1:-1], sep=' '))\n",
    "feature_test = test_feature_df['Path'].apply(lambda x: np.fromstring(x[1:-1], sep=' '))\n",
    "feature_valid = valid_feature_df['Path'].apply(lambda x: np.fromstring(x[1:-1], sep=' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U79RrHaZemqc"
   },
   "outputs": [],
   "source": [
    "X_tr=np.array(feature_train.values.tolist())\n",
    "X_train=X_tr.reshape((X_tr.shape[0],X_tr.shape[1],1))\n",
    "print(X_train.shape)\n",
    "X_val=np.array(feature_valid.values.tolist())\n",
    "X_validation=X_val.reshape((X_val.shape[0],X_val.shape[1],1))\n",
    "print(X_validation.shape)\n",
    "X_test=np.array(feature_test.values.tolist())\n",
    "X_test=X_test.reshape((X_test.shape[0],X_test.shape[1],1))\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVJAX1p_fUe0"
   },
   "source": [
    "### **PADDING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7y-WYJdc0_1x"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "df_train = pd.read_csv('df_train.csv')\n",
    "df_valid = pd.read_csv('df_valid.csv')\n",
    "df_test = pd.read_csv('df_test.csv')\n",
    "\n",
    "Y_train = df_train['Emotion'].values\n",
    "Y_train = encoder.fit_transform(np.array(Y_train).reshape(-1,1)).toarray()\n",
    "Y_valid = df_valid['Emotion'].values\n",
    "print(Y_valid.shape)\n",
    "Y_valid = encoder.fit_transform(np.array(Y_valid).reshape(-1,1)).toarray()\n",
    "\n",
    "Y_test = df_test['Emotion'].values\n",
    "Y_test = encoder.fit_transform(np.array(Y_test).reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vb1Alc4kDQvJ"
   },
   "outputs": [],
   "source": [
    "model_LSTM=keras.Sequential()\n",
    "model_LSTM.add(Conv1D(512, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "model_LSTM.add(MaxPooling1D(pool_size=2))\n",
    "model_LSTM.add(Conv1D(256, kernel_size=3, activation='relu'))\n",
    "model_LSTM.add(MaxPooling1D(pool_size=2))\n",
    "model_LSTM.add(LSTM(256,activation='tanh',return_sequences=True,dropout=0.30))\n",
    "model_LSTM.add(LSTM(64,activation='tanh',dropout=0.30))\n",
    "model_LSTM.add(Dense(32,activation = 'relu'))\n",
    "model_LSTM.add(Dense(6,activation = 'softmax'))\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "model_LSTM.compile(loss='categorical_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n",
    "model_LSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tCOfkcVJvcbk"
   },
   "outputs": [],
   "source": [
    "hist = model_LSTM.fit(X_train, Y_train,\n",
    "                   epochs=30,\n",
    "                   validation_data=(X_validation, Y_valid),\n",
    "                   batch_size=4,\n",
    "                   verbose=1)\n",
    "\n",
    "\n",
    "train_loss = hist.history['loss']\n",
    "val_loss = hist.history['val_loss']\n",
    "train_accuracy = hist.history['accuracy']\n",
    "val_accuracy = hist.history['val_accuracy']\n",
    "\n",
    "#Loss Graph\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(train_loss, label='Training Loss', color='blue')\n",
    "plt.plot(val_loss, label='Validation Loss', color='orange')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dCJwHZXeG2rg"
   },
   "outputs": [],
   "source": [
    "#Model Save\n",
    "model_LSTM.save('MODEL_NAME.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fU_HIbxDHVYF"
   },
   "outputs": [],
   "source": [
    "#Load Model\n",
    "loaded_model = load_model('MODEL_NAME.keras')\n",
    "\n",
    "test_loss, test_accuracy = loaded_model.evaluate(X_test, Y_test)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
